name: chat-agent
display_name: Chat Agent
version: 0.1.0

description: >
  Basic LLM chat agent using the public Ollama container. Provisions a single
  Linode, installs Docker via cloud-init, and runs `ollama/ollama:latest`.

capabilities:
  runtime: docker

deploy:
  target: linode
  linode:
    image: linode/ubuntu24.04
    region_default: us-chi
    type_default: g6-standard-2
    tags:
      - ai
      - chat
      - ollama
    container:
      image: ollama/ollama:latest
      internal_port: 11434
      external_port: 80
      post_start_script: |
        #!/bin/bash
        docker exec app ollama pull llama3
      health:
        type: http
        path: /api/tags
        port: 11434
        success_codes: [200]
        initial_delay_seconds: 10
        timeout_seconds: 2
        max_retries: 30

env:
  required: []
  optional:
    - name: OLLAMA_MODELS
      description: Comma separated models to pre-load, e.g. llama3

guidance:
  summary: |
    Use the Ollama HTTP API exposed on port 80. Substitute your Linode's IP or hostname below.
  examples:
    - description: List loaded models
      command: curl http://{host}/api/tags
    - description: Generate text with llama3
      command: |
        curl -X POST http://{host}/api/generate \
          -H 'Content-Type: application/json' \
          -d '{"model":"llama3","prompt":"Hello!"}'
