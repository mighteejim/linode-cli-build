# ML Pipeline

Production ML inference pipeline with batching, caching, and monitoring. Powered by PyTorch for model inference, FastAPI for REST endpoints, Redis for request caching, and Prometheus for metrics collection. Requires GPU instance for optimal performance.

## Quickstart

1. Copy `.env.example` to `.env` and fill in any required values.
2. Deploy with `linode-cli ai deploy`.
