name: ml-pipeline
display_name: ML Pipeline
version: 0.2.0
description: |
  Production ML inference pipeline with batching, caching, and monitoring.
  Powered by PyTorch for model inference, FastAPI for REST endpoints, Redis
  for request caching, and Prometheus for metrics collection.

capabilities:
  runtime: docker
  features:
    - gpu-nvidia
    - docker-optimize
    - redis

deploy:
  target: linode
  linode:
    image: linode/ubuntu22.04
    region_default: us-ord
    type_default: g2-gpu-rtx4000a4-m
    tags:
      - ai
      - ml
      - gpu
    
    container:
      image: pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime
      internal_port: 8000
      external_port: 80
      command: bash /app/start.sh
      volumes:
        - /app:/app
      env:
        REDIS_HOST: ${REDIS_HOST:-localhost}
        REDIS_PORT: ${REDIS_PORT:-6379}
        LOG_LEVEL: ${LOG_LEVEL:-info}
        BATCH_SIZE: ${BATCH_SIZE:-32}
      
      health:
        type: http
        path: /health
        port: 8000
        success_codes: [200]
        initial_delay_seconds: 60
        timeout_seconds: 10
        max_retries: 30

setup:
  files:
    - path: /app/main.py
      permissions: "0644"
      content: |
        """ML Inference Pipeline"""
        import os
        from fastapi import FastAPI
        from prometheus_client import Counter, Histogram, generate_latest
        import redis
        import torch
        import uvicorn
        
        app = FastAPI(title="ML Inference Pipeline")
        
        redis_client = redis.Redis(
            host=os.getenv("REDIS_HOST", "localhost"),
            port=int(os.getenv("REDIS_PORT", "6379"))
        )
        
        request_count = Counter('ml_requests_total', 'Total requests')
        request_duration = Histogram('ml_request_duration_seconds', 'Duration')
        
        @app.get("/health")
        def health():
            return {
                "status": "healthy",
                "gpu_available": torch.cuda.is_available()
            }
        
        @app.get("/metrics")
        def metrics():
            return generate_latest()
        
        @app.post("/predict")
        def predict(data: dict):
            request_count.inc()
            with request_duration.time():
                result = {"prediction": "example_output"}
                return result
        
        if __name__ == "__main__":
            uvicorn.run(app, host="0.0.0.0", port=8000)
    
    - path: /app/requirements.txt
      permissions: "0644"
      content: |
        fastapi
        uvicorn[standard]
        redis
        prometheus-client
    
    - path: /app/start.sh
      permissions: "0755"
      content: |
        #!/bin/bash
        set -e
        pip install -q -r /app/requirements.txt
        python /app/main.py

env:
  optional:
    - name: REDIS_HOST
      description: Redis server hostname. Default is localhost.
    - name: REDIS_PORT
      description: Redis server port. Default is 6379.
    - name: LOG_LEVEL
      description: Logging level (debug, info, warning, error). Default is info.
    - name: BATCH_SIZE
      description: Batch size for inference. Default is 32.

guidance:
  summary: |
    ML inference pipeline with GPU, Redis caching, and Prometheus metrics.
    Endpoints: /health, /metrics, /predict (POST), /docs
  examples:
    - description: Health check
      command: curl http://{host}/health
    - description: Run prediction
      command: |
        curl -X POST http://{host}/predict \
          -H 'Content-Type: application/json' \
          -d '{"input": "data"}'
