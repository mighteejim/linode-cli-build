name: llm-api
display_name: LLM API
version: 0.1.0

description: >
  Production-style text-generation inference API powered by vLLM. Exposes a
  REST endpoint compatible with OpenAI-style completions. Requires GPU instance.

capabilities:
  runtime: docker
  features:
    - gpu-nvidia
    - docker-optimize

deploy:
  target: linode
  linode:
    image: linode/ubuntu22.04
    region_default: us-mia
    type_default: g6-standard-8
    tags:
      - ai
      - llm
      - vllm
      - gpu
    container:
      image: vllm/vllm-openai:latest
      internal_port: 8000
      external_port: 80
      command: --model ${MODEL_NAME:-meta-llama/Meta-Llama-3-8B-Instruct} ${MAX_MODEL_LEN:+--max-model-len ${MAX_MODEL_LEN}}
      env:
        MODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct
        HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
      health:
        type: http
        path: /health
        port: 8000
        success_codes: [200]
        initial_delay_seconds: 180
        timeout_seconds: 10
        max_retries: 60

env:
  required:
    - name: HF_TOKEN
      description: Hugging Face token to download gated models.
  optional:
    - name: MODEL_NAME
      description: |
        HuggingFace model to load. Popular options:
        - microsoft/Phi-3-mini-4k-instruct (context: 4k)
        - mistralai/Mistral-7B-Instruct-v0.3 (context: 32k)
        - Qwen/Qwen2.5-7B-Instruct (context: 8k)
        - meta-llama/Meta-Llama-3-8B-Instruct (context: 8k, requires HF access)
    - name: MAX_MODEL_LEN
      description: |
        Maximum context length in tokens. If not set, auto-detected from model.
        Common values: 4096, 8192, 16384, 32768
        Set this if you want to limit context below the model's maximum.
    - name: VLLM_GPU_MEMORY_UTILIZATION
      description: GPU memory utilization fraction (default 0.9).

guidance:
  summary: |
    The template exposes an OpenAI-compatible REST endpoint on port 80. Replace {host} with the Linode hostname or IP.
    
    Configure the model by setting MODEL_NAME in your .env file. Popular options:
    - meta-llama/Meta-Llama-3-8B-Instruct (default, requires HF access)
    - mistralai/Mistral-7B-Instruct-v0.3 (open-source)
    - microsoft/Phi-3-mini-4k-instruct (open-source, smaller)
  examples:
    - description: Health check
      command: curl http://{host}/health
    - description: List models (OpenAI-compatible)
      command: |
        curl http://{host}/v1/models
    - description: Generate text
      command: |
        curl -X POST http://{host}/v1/completions \
          -H 'Content-Type: application/json' \
          -d '{"model":"meta-llama/Meta-Llama-3-8B-Instruct","prompt":"Once upon a time","max_tokens":50}'
