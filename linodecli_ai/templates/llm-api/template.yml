name: llm-api
display_name: LLM API
version: 0.1.0

description: >
  Production-style text-generation inference API powered by vLLM. Exposes a
  REST endpoint compatible with OpenAI-style completions.

deploy:
  target: linode
  linode:
    image: linode/ubuntu22.04
    region_default: us-mia
    type_default: g1-medium
    tags:
      - ai
      - llm
      - vllm
    container:
      image: vllm/vllm-openai:latest
      internal_port: 8000
      external_port: 80
      env:
        MODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct
      health:
        type: http
        path: /health
        port: 8000
        success_codes: [200]
        initial_delay_seconds: 20
        timeout_seconds: 3
        max_retries: 30

env:
  required:
    - name: HF_TOKEN
      description: Hugging Face token to download gated models.
  optional:
    - name: VLLM_GPU_MEMORY_UTILIZATION
      description: Override GPU memory utilization fraction.

guidance:
  summary: |
    The template exposes an OpenAI-compatible REST endpoint on port 80. Replace {host} with the Linode hostname or IP.
  examples:
    - description: Health check
      command: curl http://{host}/health
    - description: List models (OpenAI-compatible)
      command: |
        curl -X POST http://{host}/v1/models \
          -H 'Content-Type: application/json' \
          -d '{}'
